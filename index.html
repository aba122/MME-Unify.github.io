<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title> MME-Unify </title>
  <link rel="icon" href="./static/images/logo.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">
  <link rel="stylesheet" href="./static/css/video-player.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>

  <style>
    .no-sort {
        cursor: default;
        pointer-events: none;
        background-image: none !important; /* Remove the sort arrow */
    }
  </style>

</head>
<body>

  

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <img src="static/images/logo.png" style="width:1.8em;vertical-align: middle" alt="Logo"/>
            <span class="MME-Unify" style="vertical-align: middle;text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5)">MME-Unify</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle ", style="width: 100%;margin-bottom: 20px;">
            <strong> A Comprehensive Benchmark for Unified Multimodal Understanding and Generation</strong>
          </h2>
          
          <div class="is-size-5 publication-authors" style="width: 100%; margin: 15px auto;">
            <span class="author-block">
              Wulin Xie<sup style="color:#18acfb;">1</sup><sup>*</sup><sup>,</sup>
            </span>
            <span class="author-block">
              Yi-Fan Zhang<sup style="color:#18acfb;">1</sup><sup>*</sup><sup>‚Ä†</sup><sup>,</sup>
            </span>
            <span class="author-block">
              Chaoyou Fu<sup style="color:#ffac33;">4</sup><sup>,</sup>
            </span>
            <span class="author-block">
              Yang Shi<sup style="color:#9400D3;">2</sup><sup>,</sup>
            </span>
            <span class="author-block">
              Jianshu Zeng<sup style="color:#9400D3;">2</sup><sup>,</sup>
            </span>
            <span class="author-block">
              Bingyan Nie<sup style="color:#18acfb;">1</sup><sup>,</sup>
            </span>
            <span class="author-block">
              Hongkai Chen<sup style="color:#1a4ebf;">5</sup><sup>,</sup>
            </span>
            <span class="author-block">
              Zhang Zhang<sup style="color:#18acfb;">1</sup><sup>,</sup>
            </span>
            <span class="author-block">
              Liang Wang<sup style="color:#18acfb;">1</sup><sup>,</sup>
            </span>
            <span class="author-block">
              Tieniu Tan<sup style="color:#ffac33;">4</sup>
            </span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup style="color:#18acfb;">1</sup>CASIA,
            </span>
            <span class="author-block">
              <sup style="color:#9400D3;">2</sup>PKU,
            </span>
            <span class="author-block">
              <sup style="color:#ed4b82;">3</sup>UCAS,
            </span>
            <span class="author-block">
              <sup style="color:#ffac33;">4</sup>NJU,
            </span>
            <span class="author-block">
              <sup style="color:#1a4ebf;">5</sup>Vivo
            </span>
            <br>
            <span class="author-block" style="font-size:25px; text-shadow: 2px 2px 4px rgba(67, 113, 232, 0.5);">
              * Equal Contribution&nbsp;&nbsp;&nbsp;‚Ä† Project leader
            </span>
          </div>
          
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2408.13257"
                   class="external-link button is-normal is-rounded is-dark">
                <!-- <a href="https://lupantech.github.io/papers/arxiv23_mathvista.pdf"
                   class="external-link button is-normal is-rounded is-dark"> -->
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/aba122/MME-Unify"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/wulin222/MME-Unify"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-huggingface"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                  </span>
                  <span>Dataset</span>
                </a>
              </span> 

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Introduction -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top: -5%;">Introduction</h2>
        <div class="content has-text-justified">
          &nbsp;&nbsp;&nbsp;Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to:
          1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons;
          2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities.<br>

          &nbsp;&nbsp;&nbsp;We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes:<br>

          <strong>&nbsp;&nbsp;&nbsp;&nbsp;1. Standardized Traditional Task Evaluation.</strong> We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies.<br>
          
          <strong>&nbsp;&nbsp;&nbsp;&nbsp;2. Unified Task Assessment.</strong> We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning.<br>
          
          <strong>&nbsp;&nbsp;&nbsp;&nbsp;3. Comprehensive Model Benchmarking.</strong> We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, and VILA-U, alongside specialized understanding (e.g., Claude-3.5) and generation models (e.g., DALL-E-3).<br>
          
          &nbsp;&nbsp;&nbsp;Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively.
        </div>
      </div>
    </div>
    <!--/ Introduction -->
  </div>
</section>




<script>
  document.addEventListener("DOMContentLoaded", function() {
    updateRowNumbers();
    document.querySelector("#results").addEventListener("click", updateRowNumbers);
  });

  function updateRowNumbers() {
    const rows = document.querySelectorAll("#results tbody tr");
    rows.forEach((row, index) => {
      row.querySelector("td").innerText = index + 1;
    });
  }
</script>

<div class="columns is-centered">
  <div class="container">
    <div class="content has-text-centered">
      <h2 class="title is-3">Leaderboard</h2>

      <div class="box m-5">
        <div class="content has-text-centered">
          <img src="static/images/leaderboard.png" alt="leaderboard" style="max-width: 140%;"/>
          <p style="text-align: center; margin-top: 1%;">
            <strong>Comparison of multimodal models on understanding, generation, unifying tasks, and overall MME-U Score.</strong><br/>
            SIPU: Single Image Perception &amp; Understanding; 
            MITIU: Multiple &amp; Interleaved Image-Text Understanding; 
            VPU: Video Perception &amp; Understanding; 
            CIVG: Conditional Image-to-Video Generation; 
            FIR: Fine-grained Image Reconstruction; 
            TIE: Text-Guided Image Editing; 
            TIG: Text-to-Image Generation; 
            TVG: Text-to-Video Generation; 
            VP: Video Prediction; 
            IEE: Image Editing and Explaining; 
            CSQ: Common Sense Question Answering; 
            AL: Auxiliary Lines; 
            SD: SpotDiff; 
            VCoT: Visual CoT.<br/>
            <sup>*</sup> denotes U-MLLMs with the ability to generate interleaved images and texts, while <code>'-'</code> indicates that the model is unable to finish the corresponding task and 
            <span style="text-decoration: underline;">underlined</span> content signifies the best performance within a single model across all methods on this task.
          </p>
        </div>
      </div>
    </div>
  </div>
</div>




<!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista_other">
    <span class="mathvista_other" style="vertical-align: middle">Benchmark</span>
  </h1>
  </div>
</section>
            
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Data Examples</h2>
          <div class="content has-text-justified">
            <div class="box m-5">
              <!-- <div TODO: more pic> -->
              <div id="results-carousel" class="carousel results-carousel">  

 
              </div>
              <div class="content has-text-centered" style = "margin-top: 5%;">
                <img src="static/images/main.png" alt="teaser_tasks" width="120%"/>
                <p style="text-align: center;", class="mt-3">
                  <strong> Diagram of MME-Unify</strong>: Our Benchmark contains 3 main domains, covering 15 subtasks to fully evaluate unified MLLMs's understanding, generation, and unified capabilities. Especially, each unify task contains at least two QA pairs: Image multi-choice question and Text multi-choice. Moreover, VCoT task requires model to output action, location and generate image simultaneously.
                </p>
              </div>
            </div>
          </div> 

        </div>
      </div>
    </div>
        
    <div class="container">
      <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Benchmark Statistics</h2>
            <div class="content has-text-justified">
              <div class="box m-5">
                <!-- <div TODO: more pic> -->
                <div id="results-carousel" class="carousel results-carousel">  
  
   
                </div>
                <div class="content has-text-centered" style = "margin-top: 5%;">
                  <img src="static/images/Bin.png" alt="teaser_tasks" width="80%"/>
                  <p style="text-align: center;", class="mt-3">
                    <strong> A comprehensive visualization of the diverse tasks in MME-Unify</strong>: The figure illustrates the wide-ranging nature of the tasks covered in our benchmark, which spans from traditional understanding tasks to complex mixed-modality generation challenges.
                  </p>
                </div>
              </div>
            </div> 
  
          </div>
        </div>
      </div>


    <div class="columns is-centered">
      <div class="container">
        <div class="content has-text-centered">
          <h2 class="title is-3">Benchmark Comparison</h2>

          <div class="box m-5" >
            <div class="content has-text-centered">
              <img src="static/images/Comparision.png" alt="data-composition" style="max-width: 90%;"/>
              <p style="text-align: center; margin-left: auto; margin-top:1%;margin-right: auto; width: 90%;" >
                <strong>Comparison of MME-Unify and other Benchmark.</strong>:
                SIPU: Single Image Perception &amp; Understanding; 
                MITIU: Multiple &amp; Interleaved Image-Text Understanding; 
                VPU: Video Perception &amp; Understanding; 
                CIVG: Conditional Image-to-Video Generation; 
                FIR: Fine-grained Image Reconstruction; 
                TIE: Text-Guided Image Editing; 
                TIG: Text-to-Image Generation; 
                TVG: Text-to-Video Generation; 
                VP: Video Prediction; 
                IEE: Image Editing and Explaining; 
                CSQ: Common Sense Question Answering; 
                AL: Auxiliary Lines; 
                SD: SpotDiff; 
                VCoT: Visual CoT.<br/><br/>
              </p>
            </div>
          </div>

        </div>
      </div>
    </div>
    
  </div>


</section>

<!-- RESULTS SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista_other">Experiment Results</h1>
  </div>
</section>

<section class="section">
  <div class="container">

    <div class="columns is-centered">
      <div class="column has-text-centered content">
        <h2 class="title is-3">Accuracy on Visual CoT Task </h2>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/Accuracy.png" alt="e" width="70%"/>
              <p style="text-align: center; margin-left: auto; margin-right: auto; width: 70%;" >
                <strong>
                  Accuracy distribution across different dimensions on Visual CoT task
                </strong>:  
                (a) action, (b) location, and (c) image.
              </p>
            </div>
          </div>

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">


        <h2 class="title is-3">Experimental Results on All Task Splits</h2>
        <div class="box m-5">
          
          <div id="results-carousel" class="carousel results-carousel">  
          
            <div class="content has-text-centered image-wrapper">
              <img src="static/images/generation_task.png" alt="" width="90%"/>
              <p style="margin-bottom: 30px;">(1) Experimental results on various generation tasks. </p>
            </div>
          
            <div class="content has-text-centered image-wrapper">
              <img src="static/images/unify_task.png" alt="" width="100%"/>
              <p style="margin-bottom: 30px;">(2) Experimental results on various unify tasks.</p>
            </div>

          </div>

          </div>

      </div>
    </div>

  </div>
</section>


</section>
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista_other" id="citation">Citation</h1>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <pre><code>
      @article{zhang2024mme,
        title={MME-Unify: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?},
        author={Zhang, Yi-Fan and Zhang, Huanyu and Tian, Haochen and Fu, Chaoyou and Zhang, Shuangqing and Wu, Junfei and Li, Feng and Wang, Kun and Wen, Qingsong and Zhang, Zhang and others},
        journal={arXiv preprint arXiv:2408.13257},
        year={2024}
      }      
</code></pre>
  </div>
</section>


<section class="section">
  <div class="container" style="width: 60%;">
  <style>
      pre {
        background-color: #f4f4f4;
        padding: 5px; /* Ë∞ÉÊï¥padding‰∏∫5px */
        border: 1px solid #ddd;
        border-radius: 5px;
        overflow-x: auto; /* ÂÖÅËÆ∏Ê∞¥Âπ≥ÊªöÂä® */
    }
    code {
        font-family: Consolas, "Courier New", monospace;
        color: #d63384; /* ‰ª£Á†ÅÊñáÊú¨È¢úËâ≤ */
    }
  </style>


  </div>
</section>


<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align: center;">
            This website is adapted from <a href="https://video-mme.github.io/">Video-MME</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>

